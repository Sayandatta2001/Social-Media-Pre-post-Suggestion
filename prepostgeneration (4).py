# -*- coding: utf-8 -*-
"""PrePostGeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16baJkY7ehRcGxGEzRYn8yHL71Ww-6yWI
"""

!pip show transformers

# Commented out IPython magic to ensure Python compatibility.
# %env WANDB_DISABLED=true

# Colab-ready cell: LoRA adapter training for SmolLM2 (using transformers 4.55.4, torch 2.8.x)
# Installs a few helper libraries (peft, datasets, accelerate, sentence-transformers, evaluate, textstat)
# IMPORTANT: If your Colab already has some packages installed, pip may upgrade them; pin versions if you want strict behavior.

# 1) Install dependencies (will not downgrade torch/transformers if already present)
!pip install -q -U peft datasets accelerate sentence-transformers evaluate textstat

# 2) Imports and quick checks
import os
import math
import torch
from dataclasses import dataclass
from typing import Optional, Dict, List

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)
from peft import LoraConfig, TaskType, get_peft_model

from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import evaluate
import textstat

print("torch:", torch.__version__)
import transformers
print("transformers:", transformers.__version__)
import peft
print("peft:", peft.__version__)

# 3) User-configurable parameters
CSV_PATH = "/content/labeled_data.csv"  # <<-- change to your uploaded CSV path (must have 'tweet', 'hate_speech','offensive_language','neither')
MODEL_NAME = "HuggingFaceTB/SmolLM2-360M-Instruct"  # changeable; smolLM2 family model
OUTPUT_DIR = "./lora_smollm2_out"
BATCH_SIZE = 8
EPOCHS = 3
LR = 2e-4
MAX_LENGTH = 256  # max token length for prompt+target
LORA_R = 8
LORA_ALPHA = 32
LORA_DROPOUT = 0.05
SEED = 42
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 4) Small utilities: create templated target text from the numeric scores
def scores_to_label_and_text(hate_s, off_s, neither_s):
    # use the highest scoring label as main category, but keep numeric values
    vals = {"hate_speech": float(hate_s), "offensive_language": float(off_s), "neither": float(neither_s)}
    main = max(vals, key=vals.get)
    score = vals[main]
    # craft a short templated pre-post/warning text
    if main == "hate_speech":
        text = f"Warning: This post likely contains hate speech (score {score:.1f}/10). Consider rephrasing."
    elif main == "offensive_language":
        text = f"Warning: This post may contain offensive language (score {score:.1f}/10). Consider softening your wording."
    else:
        text = f"Note: This post seems benign (score {score:.1f}/10)."
    return main, text

# 5) Load tokenizer & model (we load in fp16 if GPU present)
print("Loading tokenizer and base model:", MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
# ensure pad_token exists for batching
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token": "[PAD]"})

# load model
model_kwargs = {}
if torch.cuda.is_available():
    model_kwargs["torch_dtype"] = torch.float16
# We avoid forcing device_map here to keep compatibility; Trainer will handle device placement.
base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)
# If padding token newly added, resize embeddings
if tokenizer.pad_token_id is not None and tokenizer.pad_token_id >= base_model.get_input_embeddings().weight.shape[0]:
    base_model.resize_token_embeddings(len(tokenizer))

print("Wrapping model with LoRA (PEFT)...")
lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=None,  # None => PEFT tries to auto-detect; adjust if you know module names (e.g. ["q_proj","v_proj"])
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()  # show how many params will be trained

# 6) Load CSV dataset (expects columns: tweet, hate_speech, offensive_language, neither)
if not os.path.exists(CSV_PATH):
    raise FileNotFoundError(f"CSV not found at {CSV_PATH}. Upload it to Colab and set CSV_PATH accordingly.")

ds = load_dataset("csv", data_files={"train": CSV_PATH})
# optionally split train/validation
ds = ds["train"].train_test_split(test_size=0.05, seed=SEED)
train_ds = ds["train"]
eval_ds = ds["test"]
print(f"Train size: {len(train_ds)}, Eval size: {len(eval_ds)}")

# 7) Build prompt + target and tokenize. We'll produce input = prompt + target and mask labels for prompt tokens.
PROMPT_TEMPLATE = "Tweet: {tweet}\nLabels: hate={hate:.1f}, off={off:.1f}, neither={neither:.1f}\nWarning:"

def make_example(ex):
    tweet = ex.get("tweet") or ex.get("text") or ""
    hate = float(ex.get("hate_speech", 0.0))
    off = float(ex.get("offensive_language", 0.0))
    neither = float(ex.get("neither", 0.0))
    main, target = scores_to_label_and_text(hate, off, neither)
    prompt = PROMPT_TEMPLATE.format(tweet=tweet, hate=hate, off=off, neither=neither)
    # target text begins right after "Warning:" prompt; we include a leading space to make tokenization stable
    full = prompt + " " + target
    # Tokenize whole thing
    tok = tokenizer(full, truncation=True, max_length=MAX_LENGTH, padding=False)
    prompt_tok = tokenizer(prompt, truncation=True, max_length=MAX_LENGTH, padding=False)
    # Create labels: mask prompt tokens with -100
    labels = [-100] * len(prompt_tok["input_ids"]) + tok["input_ids"][len(prompt_tok["input_ids"]):]
    # to ensure labels length matches input_ids after truncation
    labels = labels[: len(tok["input_ids"])]
    assert len(labels) == len(tok["input_ids"])
    return {
        "input_ids": tok["input_ids"],
        "attention_mask": tok["attention_mask"],
        "labels": labels,
        "meta_target": target,
        "meta_main_label": main,
        "meta_scores": {"hate": hate, "off": off, "neither": neither},
        "raw_tweet": tweet,
    }

def map_fn(batch):
    return make_example(batch)

# Map (works with Arrow Datasets)
train_tok = train_ds.map(map_fn, remove_columns=train_ds.column_names)
eval_tok = eval_ds.map(map_fn, remove_columns=eval_ds.column_names)

# 8) Data collator for causal LM with labels already prepared (pads input_ids and labels with -100 for labels)
@dataclass
class DataCollatorForCausal:
    tokenizer: AutoTokenizer
    padding: bool = True

    def __call__(self, batch: List[Dict]) -> Dict[str, torch.Tensor]:
        input_ids = [x["input_ids"] for x in batch]
        attention_mask = [x["attention_mask"] for x in batch]
        labels = [x["labels"] for x in batch]
        # pad
        b_input = self.tokenizer.pad({"input_ids": input_ids, "attention_mask": attention_mask}, return_tensors="pt")
        # pad labels manually (pad with -100)
        maxlen = b_input["input_ids"].shape[1]
        labels_padded = []
        for lab in labels:
            padded = lab + [-100] * (maxlen - len(lab))
            labels_padded.append(padded[:maxlen])
        batch_labels = torch.tensor(labels_padded, dtype=torch.long)
        b_input["labels"] = batch_labels
        return b_input

data_collator = DataCollatorForCausal(tokenizer=tokenizer)

# 9) TrainingArguments and Trainer
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LR,
    fp16=torch.cuda.is_available(),
    logging_steps=50,
    save_strategy="epoch",
    save_total_limit=2,
    seed=SEED,
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tok,
    eval_dataset=eval_tok,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# 10) Train (this trains only the LoRA adapter params)
print("Starting training. Device:", DEVICE)
trainer.train()
trainer.save_model(OUTPUT_DIR)  # saves peft adapter + base config

# 11) Post-train evaluation: generate on eval set and compute automated metrics
print("Running evaluation and computing metrics...")

# helper: generate text and compute average token log-prob (abstain proxy)
def generate_with_logprobs(model, tokenizer, prompts, max_new_tokens=64, **gen_kwargs):
    # returns list of dicts: {"generated_text": str, "avg_logprob": float, "raw_scores": ...}
    model.eval()
    results = []
    # ensure model on same device
    device = next(model.parameters()).device
    for p in prompts:
        input_ids = tokenizer(p, return_tensors="pt", truncation=True, max_length=MAX_LENGTH).input_ids.to(device)
        # generate with scores to compute token probs
        out = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            return_dict_in_generate=True,
            output_scores=True,
            do_sample=False,
            **gen_kwargs,
        )
        # reconstruct text
        gen_ids = out.sequences[0][input_ids.shape[-1]:]
        text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()
        # compute avg logprob from scores (scores: list of logits for each generated step)
        # each score corresponds to distribution for a generated token
        scores = out.scores  # list(len = gen_len) of logits tensors (1, vocab_size)
        # convert to logprob of selected tokens at each position
        import torch.nn.functional as F
        if len(scores) == 0:
            avg_lp = float("-inf")
        else:
            logps = []
            for step_idx, score in enumerate(scores):
                # score shape: (batch_size=1, vocab_size)
                probs = F.log_softmax(score[0], dim=-1)  # (vocab_size,)
                token_id = gen_ids[step_idx].item()
                logps.append(probs[token_id].item())
            avg_lp = float(sum(logps) / len(logps))
        results.append({"generated_text": text, "avg_logprob": avg_lp})
    return results

# prepare eval prompts (same prompt template we used for training, without target)
eval_prompts = []
eval_targets = []
eval_meta = []
for row in eval_ds:
    tweet = row.get("tweet") or row.get("text") or ""
    hate = float(row.get("hate_speech", 0.0))
    off = float(row.get("offensive_language", 0.0))
    neither = float(row.get("neither", 0.0))
    prompt = PROMPT_TEMPLATE.format(tweet=tweet, hate=hate, off=off, neither=neither)
    main, templ_target = scores_to_label_and_text(hate, off, neither)
    eval_prompts.append(prompt)
    eval_targets.append(templ_target)
    eval_meta.append({"hate": hate, "off": off, "neither": neither, "main": main, "raw_tweet": tweet})

# run generation in batches (to be simple: loop)
gen_results = generate_with_logprobs(model, tokenizer, eval_prompts, max_new_tokens=64)

# metrics computations
# (A) Diversity: distinct-1, distinct-2
def distinct_n(texts, n):
    total_ngrams = 0
    uniq = set()
    for t in texts:
        toks = tokenizer.tokenize(t)
        ngrams = zip(*[toks[i:] for i in range(n)])
        ngrams = list(ngrams)
        total_ngrams += len(ngrams)
        for g in ngrams:
            uniq.add(tuple(g))
    return len(uniq) / (total_ngrams + 1e-12)

gen_texts = [r["generated_text"] for r in gen_results]
distinct_1 = distinct_n(gen_texts, 1)
distinct_2 = distinct_n(gen_texts, 2)

# (B) Readability: mean Flesch reading ease (textstat)
readabilities = [textstat.flesch_reading_ease(t) if len(t.strip())>0 else 0.0 for t in gen_texts]
mean_readability = sum(readabilities)/len(readabilities)

# (C) Embedding similarity (sentence-transformers) between templated target and generated text
sbert = SentenceTransformer("all-MiniLM-L6-v2")  # fairly lightweight
target_embs = sbert.encode(eval_targets, convert_to_tensor=True)
gen_embs = sbert.encode(gen_texts, convert_to_tensor=True)
import torch
cos_sims = torch.nn.functional.cosine_similarity(torch.tensor(target_embs), torch.tensor(gen_embs))
mean_cosine = float(cos_sims.mean().item())

# (D) Average generation avg_logprob (abstain proxy), and compute abstain rate under threshold
avg_logprobs = [r["avg_logprob"] for r in gen_results]
mean_avg_logprob = sum([v for v in avg_logprobs if math.isfinite(v)]) / sum([1 for v in avg_logprobs if math.isfinite(v)])
# simple abstain rule: avg token logprob < threshold => abstain
ABSTAIN_LOGPROB_THRESHOLD = -6.5
abstain_count = sum(1 for v in avg_logprobs if (not math.isfinite(v)) or v < ABSTAIN_LOGPROB_THRESHOLD)
abstain_rate = abstain_count / len(avg_logprobs)

# (E) Policy coverage: when hate_speech highest, does generated warning mention 'hate'?
def contains_hate_word(text):
    t = text.lower()
    return ("hate" in t) or ("hate speech" in t) or ("offensive" in t)  # simple heuristic

policy_hits = 0
policy_applicable = 0
for meta, gen in zip(eval_meta, gen_texts):
    # if hate_speech score is main label (we consider it a high-risk slice)
    if meta["main"] == "hate_speech":
        policy_applicable += 1
        if contains_hate_word(gen):
            policy_hits += 1
policy_coverage = (policy_hits / policy_applicable) if policy_applicable>0 else None

# (F) Per-example small report
sample_report = []
for i in range(min(6, len(gen_texts))):
    sample_report.append({
        "tweet": eval_meta[i]["raw_tweet"][:200],
        "target": eval_targets[i],
        "generated": gen_texts[i],
        "avg_logprob": avg_logprobs[i],
        "meta_scores": eval_meta[i],
    })

# 12) Print summary metrics
print("\n=== Evaluation summary ===")
print(f"Eval examples: {len(gen_texts)}")
print(f"Distinct-1: {distinct_1:.4f}, Distinct-2: {distinct_2:.4f}")
print(f"Mean readability (Flesch reading ease): {mean_readability:.2f}  (higher => easier to read)")
print(f"Mean cosine similarity to templated target (sbert): {mean_cosine:.4f}")
print(f"Mean avg token logprob (generated): {mean_avg_logprob:.4f}")
print(f"Abstain threshold (avg_logprob < {ABSTAIN_LOGPROB_THRESHOLD}): abstain_rate = {abstain_rate:.3f}")
if policy_coverage is not None:
    print(f"Policy-coverage on hate_speech slice: {policy_coverage:.3f} ({policy_hits}/{policy_applicable})")
else:
    print("Policy-coverage: no examples in hate_speech slice in eval set.")
print("\nSample outputs:")
for s in sample_report:
    print("-"*30)
    print("Tweet:", s["tweet"])
    print("Target:", s["target"])
    print("Generated:", s["generated"])
    print("avg_logprob:", s["avg_logprob"])
    print("scores:", s["meta_scores"])

# 13) Save adapter (already saved by trainer) and optionally push to hub (requires auth)
print(f"\nLoRA adapter and trainer checkpoints saved to {OUTPUT_DIR}.")

# Colab Cell 1: Plot training loss and many evaluation plots
# Paste/run this as a separate Colab cell after your training cell (it expects the training objects/variables to still be in memory).
# It will try multiple fallbacks (use `trainer.state.log_history` if available; otherwise try to load trainer_state.json).
# It also expects lists/variables produced by the evaluation step in the previous cell (gen_texts, eval_targets,
# avg_logprobs, readabilities, cos_sims, eval_meta). If those are not present it will try to load OUTPUT_DIR/eval_results.json
# (not created by default) — but it will still produce loss plots from trainer logs if available.
#
# This cell uses matplotlib (one figure per plot) and pandas. It saves plots to OUTPUT_DIR/plots.
# It also displays a small sample DataFrame of generated examples (uses caas_jupyter_tools.display_dataframe_to_user).

import os, json, math
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch

# helper to locate OUTPUT_DIR if not defined
OUTPUT_DIR = globals().get("OUTPUT_DIR", "./lora_smollm2_out")
PLOT_DIR = os.path.join(OUTPUT_DIR, "plots")
os.makedirs(PLOT_DIR, exist_ok=True)

# 1) Load training log / trainer.state.log_history
log_history = None
trainer = globals().get("trainer", None)
if trainer is not None and hasattr(trainer, "state"):
    log_history = trainer.state.log_history

# fallback: load trainer_state.json saved by Trainer
if log_history is None:
    ts_path = os.path.join(OUTPUT_DIR, "trainer_state.json")
    if os.path.exists(ts_path):
        try:
            with open(ts_path, "r") as f:
                ts = json.load(f)
            # TrainerState contains log_history as a list maybe under "log_history"
            log_history = ts.get("log_history", None) or ts.get("log_history", [])
        except Exception as e:
            print("Failed to read trainer_state.json:", e)
            log_history = None

# If still None, try trainer_state saved in output_dir as events or logs (rare)
if log_history is None:
    print("No trainer log history found in memory or in", OUTPUT_DIR)
    log_history = []

# Convert log_history into DataFrame
df_logs = pd.DataFrame(log_history)
if not df_logs.empty:
    # normalize keys present: step, loss, epoch, eval_loss, learning_rate, epoch_time etc
    # make sure 'step' column exists
    if 'step' not in df_logs.columns and 'total_flos' in df_logs.columns:
        df_logs['step'] = df_logs.index
else:
    print("log_history is empty; won't plot training loss vs steps from trainer.")

# 2) Build eval results dataframe from variables computed earlier if present
# Try to use variables from the previous run (gen_texts, eval_targets, avg_logprobs, readabilities, cos_sims, eval_meta)
gen_texts = globals().get("gen_texts", None)
eval_targets = globals().get("eval_targets", None)
avg_logprobs = globals().get("avg_logprobs", None)
readabilities = globals().get("readabilities", None)
cos_sims = globals().get("cos_sims", None)
eval_meta = globals().get("eval_meta", None)

# If cos_sims is tensor, convert to numpy list
if isinstance(cos_sims, torch.Tensor):
    cos_sims = cos_sims.detach().cpu().numpy().tolist()

# If nothing in memory, try to load eval_results.json in OUTPUT_DIR
if gen_texts is None or avg_logprobs is None:
    eval_json_path = os.path.join(OUTPUT_DIR, "eval_results.json")
    if os.path.exists(eval_json_path):
        try:
            with open(eval_json_path, "r") as f:
                loaded = json.load(f)
            gen_texts = loaded.get("gen_texts", gen_texts)
            eval_targets = loaded.get("eval_targets", eval_targets)
            avg_logprobs = loaded.get("avg_logprobs", avg_logprobs)
            readabilities = loaded.get("readabilities", readabilities)
            cos_sims = loaded.get("cos_sims", cos_sims)
            eval_meta = loaded.get("eval_meta", eval_meta)
            print("Loaded eval results from", eval_json_path)
        except Exception as e:
            print("Failed to load eval_results.json:", e)

# Guard: if still None create empty lists to avoid crashes
gen_texts = gen_texts or []
eval_targets = eval_targets or []
avg_logprobs = avg_logprobs or []
readabilities = readabilities or []
cos_sims = cos_sims or []
eval_meta = eval_meta or []

# Build per-example DataFrame if lengths match
min_len = min(len(gen_texts), len(eval_targets), len(avg_logprobs) or len(gen_texts), len(readabilities) or len(gen_texts), len(cos_sims) or len(gen_texts))
rows = []
for i in range(min_len):
    meta = eval_meta[i] if i < len(eval_meta) else {}
    rows.append({
        "idx": i,
        "tweet": (meta.get("raw_tweet") if isinstance(meta, dict) else "")[:200],
        "target": eval_targets[i] if i < len(eval_targets) else "",
        "generated": gen_texts[i],
        "avg_logprob": float(avg_logprobs[i]) if i < len(avg_logprobs) else np.nan,
        "readability": float(readabilities[i]) if i < len(readabilities) else np.nan,
        "cosine": float(cos_sims[i]) if i < len(cos_sims) else np.nan,
        "main_label": (meta.get("main") if isinstance(meta, dict) else None),
        "hate": meta.get("hate") if isinstance(meta, dict) else None,
        "off": meta.get("off") if isinstance(meta, dict) else None,
        "neither": meta.get("neither") if isinstance(meta, dict) else None,
    })
df_eval = pd.DataFrame(rows)

# --- PLOTS ---
# 1) Training loss vs step
if not df_logs.empty and 'loss' in df_logs.columns:
    plt.figure()
    plt.plot(df_logs['step'], df_logs['loss'], marker='o', linestyle='-')
    plt.xlabel("Training step")
    plt.ylabel("Training loss")
    plt.title("Training loss vs step")
    plt.grid(True)
    p = os.path.join(PLOT_DIR, "loss_vs_step.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)
else:
    print("No 'loss' entries in trainer log history to plot.")

# 2) Eval loss vs epoch (if present)
if not df_logs.empty and 'eval_loss' in df_logs.columns:
    df_evalloss = df_logs.dropna(subset=['eval_loss'])
    plt.figure()
    plt.plot(df_evalloss['epoch'], df_evalloss['eval_loss'], marker='o', linestyle='-')
    plt.xlabel("Epoch")
    plt.ylabel("Eval loss")
    plt.title("Evaluation loss vs epoch")
    plt.grid(True)
    p = os.path.join(PLOT_DIR, "eval_loss_vs_epoch.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)

# 3) Histogram of avg_token_logprob
if len(df_eval)>0:
    plt.figure()
    plt.hist(df_eval["avg_logprob"].dropna(), bins=30)
    plt.xlabel("Avg token logprob")
    plt.ylabel("Count")
    plt.title("Distribution of avg token logprob (confidence proxy)")
    plt.grid(True)
    p = os.path.join(PLOT_DIR, "hist_avg_logprob.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)

# 4) Histogram of cosine similarity
if "cosine" in df_eval.columns and df_eval["cosine"].notna().any():
    plt.figure()
    plt.hist(df_eval["cosine"].dropna(), bins=30)
    plt.xlabel("Cosine similarity (sbert)")
    plt.ylabel("Count")
    plt.title("Distribution of cosine similarity between generated and templated target")
    plt.grid(True)
    p = os.path.join(PLOT_DIR, "hist_cosine_similarity.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)

# 5) Histogram of readability
if "readability" in df_eval.columns and df_eval["readability"].notna().any():
    plt.figure()
    plt.hist(df_eval["readability"].dropna(), bins=25)
    plt.xlabel("Flesch reading ease")
    plt.ylabel("Count")
    plt.title("Readability distribution of generated warnings")
    plt.grid(True)
    p = os.path.join(PLOT_DIR, "hist_readability.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)

# 6) Scatter: cosine vs avg_logprob
if ("cosine" in df_eval.columns and "avg_logprob" in df_eval.columns and
        df_eval["cosine"].notna().any() and df_eval["avg_logprob"].notna().any()):
    plt.figure()
    plt.scatter(df_eval["cosine"], df_eval["avg_logprob"])
    plt.xlabel("Cosine similarity")
    plt.ylabel("Avg token logprob")
    plt.title("Cosine vs Avg token logprob")
    plt.grid(True)
    p = os.path.join(PLOT_DIR, "scatter_cosine_vs_logprob.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)

# 7) Boxplot: avg_logprob by main_label
if "main_label" in df_eval.columns and df_eval["main_label"].notna().any():
    unique_labels = df_eval["main_label"].dropna().unique()
    groups = [df_eval[df_eval["main_label"]==lab]["avg_logprob"].dropna() for lab in unique_labels]
    plt.figure()
    plt.boxplot(groups, labels=unique_labels)
    plt.xlabel("Main label")
    plt.ylabel("Avg token logprob")
    plt.title("Avg logprob distribution by main label")
    p = os.path.join(PLOT_DIR, "box_logprob_by_label.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)

# 8) Bar: counts per main_label and mean cosine per label
if "main_label" in df_eval.columns and df_eval["main_label"].notna().any():
    counts = df_eval["main_label"].value_counts()
    plt.figure()
    counts.plot(kind="bar")
    plt.xlabel("Main label")
    plt.ylabel("Count")
    plt.title("Counts per main label in eval set")
    p = os.path.join(PLOT_DIR, "bar_counts_main_label.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)

    # mean cosine per label
    mean_cos_per_label = df_eval.groupby("main_label")["cosine"].mean().dropna()
    plt.figure()
    mean_cos_per_label.plot(kind="bar")
    plt.xlabel("Main label")
    plt.ylabel("Mean cosine similarity")
    plt.title("Mean cosine similarity per label")
    p = os.path.join(PLOT_DIR, "bar_mean_cosine_by_label.png")
    plt.savefig(p, bbox_inches='tight')
    plt.show()
    print("Saved plot:", p)

# 9) Top-K examples by lowest avg_logprob (most uncertain) -- display a small table
if len(df_eval)>0:
    top_uncertain = df_eval.sort_values("avg_logprob").head(10)[["idx","tweet","target","generated","avg_logprob","main_label"]]
    try:
        # display with the interactive table helper if available
        from caas_jupyter_tools import display_dataframe_to_user
        display_dataframe_to_user("Top uncertain examples", top_uncertain)
    except Exception:
        print("Top uncertain examples:")
        print(top_uncertain.to_string(index=False))

# 10) Save the per-example dataframe to CSV for downstream analysis
csv_out = os.path.join(OUTPUT_DIR, "eval_per_example.csv")
df_eval.to_csv(csv_out, index=False)
print("Saved per-example eval CSV to", csv_out)

print("All plots saved to", PLOT_DIR)